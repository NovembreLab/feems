{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('feems': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b427e6b764d3ebce4814b0eea73cf2f68d5eddd2be147e79b42a1160529f9620"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Search for long range edges \n",
    "\n",
    "In this notebook, I will implement code to search for a long range edge(s) from a default `feems` by seeing if the likelihood increases with the addition of a certain edge. First, the search for the *best* edge will be done through a full grid search, wherein I iteratively add one long range edge after another over all pairs of nodes and check if this extra edge decreases the negative log likelihood. Second, I will implement a heuristic search by using a greedy approach to fit an edge between a pair of nodes that show maximum residuals with the default fit. \n",
    "\n",
    "To do this, I need to simulate a large empirical test case with many nodes. I will use the same corridor-barrier-corridor approach from previous simulations but with a much larger grid (40x80 is too large, for instance, 10x50 took 2 days to only get through ~1% of all edges). Keep total number of sampled nodes to about 40 maybe? \n",
    "\n",
    "## Changes to original code base\n",
    "\n",
    "1. added code to calculate the negative log likelihood value for fit in `spatial_graph.py` (basically, adding `Objective` functions)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pkg_resources\n",
    "import itertools as it\n",
    "import math\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import statsmodels.api as sm\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# feems\n",
    "from feems.utils import prepare_graph_inputs\n",
    "from feems import SpatialGraph, Viz, Objective\n",
    "from feems.sim import setup_graph, setup_graph_long_range, simulate_genotypes\n",
    "from feems.spatial_graph import query_node_attributes\n",
    "from feems.objective import comp_mats\n",
    "from feems.cross_validation import run_cv\n",
    "from feems.helper_funcs import plot_default_vs_long_range, comp_genetic_vs_fitted_distance, plot_estimated_vs_simulated_edges\n",
    "\n",
    "# change matplotlib fonts\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"font.sans-serif\"] = \"Arial\""
   ]
  },
  {
   "source": [
    "## Simulation test case\n",
    "\n",
    "### Exhaustive search \n",
    "\n",
    "This requires a search over a little less than $\\sim \\frac{d^2}{2}$ edges (for 128 nodes, it is about 7,800 edges). Using some kind of multithreading since this problem is embarassingly parellel. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Simulating ~SNP 0\n",
      "Simulating ~SNP 50\n",
      "Simulating ~SNP 100\n",
      "Simulating ~SNP 150\n",
      "Simulating ~SNP 200\n",
      "Simulating ~SNP 250\n",
      "Simulating ~SNP 300\n",
      "Simulating ~SNP 350\n",
      "Simulating ~SNP 400\n",
      "Simulating ~SNP 450\n",
      "Simulating ~SNP 500\n",
      "Simulating ~SNP 550\n",
      "Simulating ~SNP 600\n",
      "Simulating ~SNP 650\n",
      "Simulating ~SNP 700\n",
      "Simulating ~SNP 750\n",
      "Simulating ~SNP 800\n",
      "Simulating ~SNP 850\n",
      "Simulating ~SNP 900\n",
      "Simulating ~SNP 950\n"
     ]
    }
   ],
   "source": [
    "n_rows, n_columns = 8, 16\n",
    "graph_def, _, _, edge_def = setup_graph(n_rows=n_rows, n_columns=n_columns, barrier_startpt=2.5, barrier_endpt=6.5, corridor_w=0.5, barrier_w=0.1, barrier_prob=1.0)\n",
    "\n",
    "lrn = [(0,38)]\n",
    "\n",
    "## using 1.0 to ensure all nodes are sampled equally well (default params otherwise: 4x8 grid)\n",
    "graph, coord, grid, edge = setup_graph_long_range(n_rows=n_rows, n_columns=n_columns, corridor_w=1.0, barrier_w=0.5, barrier_prob=1.0, long_range_nodes=lrn, long_range_edges=[1.5])\n",
    "\n",
    "gen_test = simulate_genotypes(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_Graph_def = SpatialGraph(gen_test, coord, grid, edge_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all edges to add (since it is symmetric, we should expect d(d-1)/2 but some will be adjacent nodes so fewer than that)\n",
    "lr = (tuple(i) for i in it.product(tuple(range(sp_Graph_def.n_observed_nodes)), repeat=2) if tuple(reversed(i)) > tuple(i))\n",
    "final_lr = [x for x in list(lr) if x not in list(sp_Graph_def.edges)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index = np.arange(len(final_lr)), columns = ['nodes', 'nll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in an edge, adds to default graph and returns negative log-lik\n",
    "def add_edge(val):\n",
    "    edges_lr = deepcopy(edge_def)\n",
    "    edges_lr = edges_lr.tolist()\n",
    "    edges_lr.append(list(x+1 for x in val))\n",
    "    sp_Graph = SpatialGraph(gen_test, coord, grid, np.array(edges_lr))\n",
    "    sp_Graph.fit(lamb = 10.0, verbose=False)\n",
    "    return sp_Graph.nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 7h 25min 18s, sys: 4min 15s, total: 7h 29min 34s\nWall time: 3h 39min 5s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 38)"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "%%time\n",
    "# for idx, val in enumerate(final_lr):\n",
    "#     # creating a new edge vector (all default edges + 1 long range)\n",
    "#     edges_lr = deepcopy(edge_def)\n",
    "#     edges_lr = edges_lr.tolist()\n",
    "#     edges_lr.append(list(x+1 for x in val))\n",
    "#     sp_Graph = SpatialGraph(gen_test, coord, grid, np.array(edges_lr))\n",
    "#     sp_Graph.fit(lamb = 10.0, verbose=False)\n",
    "\n",
    "#     df.iloc[idx, 0] = val\n",
    "#     df.iloc[idx, 1] = sp_Graph.nll\n",
    "\n",
    "# tbh no difference in timings between above for loop and one-liner map\n",
    "df['nodes'] = final_lr\n",
    "df['nll'] = list(map(add_edge, df.iloc[np.arange(len(final_lr)),0]))\n",
    "\n",
    "# print nodes connected by THE edge to give lowest negative log likelihood\n",
    "df.loc[df['nll'].astype(float).idxmin(),'nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 5min 33s, sys: 4.04 s, total: 5min 37s\nWall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # sequential run for timing purposes\n",
    "# df_test = pd.DataFrame(index = np.arange(100), columns = ['nodes', 'nll'])\n",
    "# for idx, val in enumerate(final_lr[0:100]):\n",
    "#     df_test.iloc[idx, 0] = val\n",
    "#     df_test.iloc[idx, 1] = add_edge(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       nodes           nll\n",
       "0     (0, 2)  13988.335454\n",
       "1     (0, 3)  13951.909591\n",
       "2     (0, 4)  13895.398119\n",
       "3     (0, 5)  13854.259583\n",
       "4     (0, 6)  13861.783873\n",
       "..       ...           ...\n",
       "95   (0, 98)  13950.784067\n",
       "96   (0, 99)  13976.136087\n",
       "97  (0, 100)   13929.84991\n",
       "98  (0, 101)  13870.904828\n",
       "99  (0, 102)   13878.63956\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nodes</th>\n      <th>nll</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(0, 2)</td>\n      <td>13988.335454</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(0, 3)</td>\n      <td>13951.909591</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(0, 4)</td>\n      <td>13895.398119</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(0, 5)</td>\n      <td>13854.259583</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(0, 6)</td>\n      <td>13861.783873</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>(0, 98)</td>\n      <td>13950.784067</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>(0, 99)</td>\n      <td>13976.136087</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>(0, 100)</td>\n      <td>13929.84991</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>(0, 101)</td>\n      <td>13870.904828</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>(0, 102)</td>\n      <td>13878.63956</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 5min 2s, sys: 30.9 s, total: 5min 33s\nWall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test['nodes'] = df.iloc[0:100,0]\n",
    "df_test['nll'] = list(map(add_edge, df_test.iloc[0:100,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/feems/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/feems/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/feems/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/feems/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/feems/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # this section of code takes forever, some overhead in communication potentially\n",
    "# pool = Pool(processes=mp.cpu_count())\n",
    "\n",
    "# nll = pool.map(add_edge, (val for val in df_test['nodes']))\n",
    "\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# df_test['nll'] = nll"
   ]
  },
  {
   "source": [
    "Based on initial testing on the simulations, I find that an exhaustive search works well in recovering the long range edge based on maximum decrease in negative log-likelihood. \n",
    "\n",
    "### Implementing a heuristic seach\n",
    "\n",
    "We can use a data-driven apporach as before to find the pairs of nodes with highest residuals, and then search in a set region around the node. For instance, let's say, we get back (1,40) as the pair of nodes. We then implement a search in which we add edges between (0,40), (1,40), (2,40), ..., (2,41), etc. In this simple scheme, I will choose the six closest neighbors (forming a hexagon, if an interior node) to the selected nodes in the pair. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "constant-w/variance fit, converged in 123 iterations, train_loss=13990.5887471\nlambda=10.0000000, alpha=0.6633522, converged in 14 iterations, train_loss=12458.5572596\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-36b964c76e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_res_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_genetic_vs_fitted_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_Graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotFig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_Graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_Graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_res_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# obtaining pairs of nodes with maximum residuals (TOP 1)\n",
    "max_res_nodes = comp_genetic_vs_fitted_distance(sp_Graph_def, n_lre=1, lamb=10.0, plotFig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting closest neighbors to each node with maximum residuals\n",
    "n1 = list(sp_Graph_def[max_res_nodes[0][0]])\n",
    "n2 = list(sp_Graph_def[max_res_nodes[0][1]])\n",
    "\n",
    "n1.append(max_res_nodes[0][0])\n",
    "n2.append(max_res_nodes[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_heur = (tuple(i) for i in it.product(n1, n2))\n",
    "# removing nodes that are already connected in the default graph \n",
    "# TODO: introduce a measure here to remove nodes that are close by on the graph\n",
    "final_lr_heur = [x for x in list(lr_heur) if x not in list(sp_Graph_def.edges)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 2s, sys: 6.79 s, total: 1min 9s\nWall time: 35.5 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 38)"
      ]
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "source": [
    "%%time\n",
    "df_heur = pd.DataFrame(index = np.arange(len(final_lr_heur)), columns = ['nodes', 'nll'])\n",
    "\n",
    "df_heur['nodes'] = final_lr_heur\n",
    "df_heur['nll'] = list(map(add_edge, df_heur.iloc[np.arange(len(final_lr_heur)),0]))\n",
    "\n",
    "# print nodes connected by THE edge to give lowest negative log likelihood\n",
    "df_heur.loc[df_heur['nll'].astype(float).idxmin(),'nodes']"
   ]
  },
  {
   "source": [
    "### Code to create a convex hull around nodes\n",
    "\n",
    "Below I will write some code to create a search region around the selected nodes in which I will incorporate nodes to use in the search. Important use cases to keep in mind:  \n",
    "1. ensure that the two convex hulls are non-overlapping (i.e., at least 2 neighbors apart)  \n",
    "2. set a manual threshold if the closest sampled node is too far  \n",
    "\n",
    "#### Simulating a use-case different from the previous "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}